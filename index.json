
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Sandhya Tripathi is a researcher specializing in machine learning applications within healthcare. Her research interests span perioperative and critical care settings, focusing on the creation of AI-based clinical decision support systems—from designing AI models to deploying them in EHR systems—and exploring how these models impact different societal demographics.\n","date":1693180800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1693180800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Sandhya Tripathi is a researcher specializing in machine learning applications within healthcare. Her research interests span perioperative and critical care settings, focusing on the creation of AI-based clinical decision support systems—from designing AI models to deploying them in EHR systems—and exploring how these models impact different societal demographics.","tags":null,"title":"Sandhya Tripathi","type":"authors"},{"authors":["Sandhya Tripathi","Christopher R King"],"categories":null,"content":"","date":1702684800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702684800,"objectID":"f5f24e77e6cb6eb02e6f8d3ed677f64a","permalink":"https://sandhyat.github.io/talk/contrastive-learning-big-data-foundations-and-applications/","publishdate":"2023-12-16T00:00:00Z","relpermalink":"/talk/contrastive-learning-big-data-foundations-and-applications/","section":"event","summary":"This was a tutorial given at BigData'2023 and CODS-COMAD'2024 conference.","tags":null,"title":"Contrastive Learning: Big Data Foundations and Applications","type":"event"},{"authors":null,"categories":null,"content":"An applied problem facing all areas of data science is harmonizing data sources. Joining data from multiple origins with unmapped and only partially overlapping features is a prerequisite to developing and testing robust, generalizable algorithms, especially in health care. This joining is usually resolved using meta-data, which may be unavailable or ambiguous in a large database. We design and evaluate methods for mapping features between databases independent of meta-data.\n","date":1701993600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701993600,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://sandhyat.github.io/project/example/","publishdate":"2023-12-08T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"The project uses fundamental statistical and deep learning techniques to solve the matching problem in EHR.","tags":["Deep Learning"],"title":"Matching Electronic Health Records from different sources","type":"project"},{"authors":["Sandhya Tripathi","Bradley A Fritz","Mohamed Abdelhack","Michael S Avidan","Yixin Chen","Christopher R King"],"categories":null,"content":"","date":1693180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1693180800,"objectID":"7da5fd4f37c46e616a3436f5867b1fac","permalink":"https://sandhyat.github.io/publication/preprint_schemamatching1/","publishdate":"2023-08-28T00:00:00Z","relpermalink":"/publication/preprint_schemamatching1/","section":"publication","summary":"An applied problem facing all areas of data science is harmonizing data sources. Joining data from multiple origins with unmapped and only partially overlapping features is a prerequisite to developing and testing robust, generalizable algorithms, especially in health care. This joining is usually resolved using meta-data, which may be unavailable or ambiguous in a large database. We design and evaluate methods for mapping features between databases independent of meta-data.  We evaluate methods in the challenging case of numeric features without reliable and distinctive univariate summaries, such as nearly Gaussian and binary features. We assume that a small set of features are a priori mapped between two databases, which share unknown identical features and possibly many unrelated features. We compare the performance of contrastive learning methods for feature representations, novel partial auto-encoders, mutual-information graph optimizers, and simple statistical baselines on simulated data, public datasets, the MIMIC-III medical-record changeover, and perioperative records from before and after a medical-record system change. Performance was evaluated using both mapping of identical features and reconstruction accuracy of examples in the format of the other database. Contrastive learning-based methods overall performed the best, often substantially beating the literature baseline in matching and reconstruction, especially in the more challenging real data experiments.  Partial auto-encoder (chimeric) method showed on par matching with contrastive methods in all synthetic and some real datasets, along with good reconstruction.  However, the statistical method we created performed reasonably well in many cases, with much less dependence on hyperparameter tuning. We also validated the matches in EHR datasets and found that some mistakes were actually a surrogate or related features as reviewed by two subject matter experts. In simulation studies and several real-world examples, we find that summaries of inter-feature relationships are effective at identifying matching or closely related features across databases when meta-data is not available. Decoder architectures are also reasonably effective at imputing features without an exact match.","tags":null,"title":"Multi-View Representation Learning to Schema Match Databases","type":"publication"},{"authors":null,"categories":null,"content":"","date":1691625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691625600,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://sandhyat.github.io/project/external-project/","publishdate":"2023-08-10T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"Use of Geographical Information Systems (GIS) for analyzing WIC (Supplemental Nutrition Program for Women, Infant and Children) office locations in Missouri.","tags":["Demo"],"title":"Where and What of WIC in Missouri: A thematic analysis","type":"project"},{"authors":["Mohamed Abdelhack","Jiaming Zhang","Sandhya Tripathi","Bradley Fritz","Daniel Felsky","Michael Avidan","Yixin Chen","Christopher Ryan King"],"categories":null,"content":"","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680307200,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://sandhyat.github.io/publication/journal-article/","publishdate":"2023-04-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Data missingness and quality are common problems in machine learning, especially for high-stakes applications such as healthcare. Developers often train machine learning models on carefully curated datasets using only high quality data; however, this reduces the utility of such models in production environments. We propose a novel neural network modification to mitigate the impacts of low quality and missing data which involves replacing the fixed weights of a fully-connected layer with a function of an additional input. This is inspired from neuromodulation in biological neural networks where the cortex can up- and down-regulate inputs based on their reliability and the presence of other data. In testing, with reliability scores as a modulating signal, models with modulating layers were found to be more robust against degradation of data quality, including additional missingness. These models are superior to imputation as they save on training time by completely skipping the imputation process and further allow the introduction of other data quality measures that imputation cannot handle. Our results suggest that explicitly accounting for reduced information quality with a modulating fully connected layer can enable the deployment of artificial intelligence systems in real-time applications.","tags":null,"title":"A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues","type":"publication"},{"authors":["Sandhya Tripathi","Bradley A Fritz","Michael S Avidan","Yixin Chen","Christopher R King"],"categories":null,"content":"","date":1669593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669593600,"objectID":"54db35530773a630679e7ecc9f8e1acc","permalink":"https://sandhyat.github.io/publication/conference-paper5/","publishdate":"2022-11-28T00:00:00Z","relpermalink":"/publication/conference-paper5/","section":"publication","summary":"Although prediction models for delirium, a commonly occurring condition during general hospitalization or post-surgery, have not gained huge popularity, their algorithmic bias evaluation is crucial due to the existing association between social determinants of health and delirium risk. In this context, using MIMIC-III and another academic hospital dataset, we present some initial experimental evidence showing how sociodemographic features such as sex and race can impact the model performance across subgroups. With this work, our intent is to initiate a discussion about the intersectionality effects of old age, race and socioeconomic factors on the early-stage detection and prevention of delirium using ML.","tags":[],"title":"Algorithmic Bias in Machine Learning Based Delirium Prediction","type":"publication"},{"authors":null,"categories":null,"content":"With the much needed discussion about fairness, explainability and transparency of machine learning models, their application in high-impact clinical decision-making systems must be scrutinized. We consider a real-life example of risk estimation before surgery and investigate the potential for bias or unfairness of a variety of algorithms. Our approach creates transparent documentation of potential bias so that the users can apply the model carefully. We augment a model-card like analysis using propensity scores with a decision-tree based guide for clinicians that would identify predictable shortcomings of the model. In addition to functioning as a guide for users, we propose that it can guide the algorithm development and informatics team to focus on data sources and structures that can address these shortcomings.\n","date":1669507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669507200,"objectID":"05b9a4a59e748925a24e75707b8a431d","permalink":"https://sandhyat.github.io/project/example-1/","publishdate":"2022-11-27T00:00:00Z","relpermalink":"/project/example-1/","section":"project","summary":"The project evaluates the clinical prediction models for fairness and provides a applicability based solution in the form of model cards.","tags":["Deep Learning","Machine Learning"],"title":"Fairness evaluation methods in clinical prediction model","type":"project"},{"authors":["Sandhya Tripathi","Bradley A Fritz","Mohamed Abdelhack","Michael S Avidan","Yixin Chen","Christopher R King"],"categories":null,"content":"","date":1655856000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1655856000,"objectID":"2f4d4fa9f70f59a920e670d2b9cbc8cf","permalink":"https://sandhyat.github.io/publication/preprint_schema_matching/","publishdate":"2022-06-22T00:00:00Z","relpermalink":"/publication/preprint_schema_matching/","section":"publication","summary":"An applied problem facing all areas of data science is harmonizing data sources. Joining data from multiple origins with unmapped and only partially overlapping features is a prerequisite to developing and testing robust, generalizable algorithms, especially in health care. We approach this issue in the common but difficult case of numeric features such as nearly Gaussian and binary features, where unit changes and variable shift make simple matching of univariate summaries unsuccessful. We develop two novel procedures to address this problem. First, we demonstrate multiple methods of \"fingerprinting\" a feature based on its associations to other features. In the setting of even modest prior information, this allows most shared features to be accurately identified. Second, we demonstrate a deep learning algorithm for translation between databases. Unlike prior approaches, our algorithm takes advantage of discovered mappings while identifying surrogates for unshared features and learning transformations. In synthetic and real-world experiments using two electronic health record databases, our algorithms outperform existing baselines for matching variable sets, while jointly learning to impute unshared or transformed variables.","tags":null,"title":"Deep Learning to Jointly Schema Match, Impute, and Transform Databases","type":"publication"},{"authors":["Sandhya Tripathi","吳恩達"],"categories":["Demo","教程"],"content":"import libr print(\u0026#39;hello\u0026#39;) Overview The Wowchemy website builder for Hugo, along with its starter templates, is designed for professional creators, educators, and teams/organizations - although it can be used to create any kind of site The template can be modified and customised to suit your needs. It’s a good platform for anyone looking to take control of their data and online identity whilst having the convenience to start off with a no-code solution (write in Markdown and customize with YAML parameters) and having flexibility to later add even deeper personalization with HTML and CSS You can work with all your favourite tools and apps with hundreds of plugins and integrations to speed up your workflows, interact with your readers, and much more Get Started 👉 Create a new site 📚 Personalize your site 💬 Chat with the Wowchemy community or Hugo community 🐦 Twitter: @wowchemy @GeorgeCushen #MadeWithWowchemy 💡 Request a feature or report a bug for Wowchemy ⬆️ Updating Wowchemy? View the Update Tutorial and Release Notes Crowd-funded open-source software To help us develop this template and software sustainably under the MIT license, we ask all individuals and businesses that use it to help support its ongoing maintenance and development via sponsorship.\n❤️ Click here to become a sponsor and help support Wowchemy’s future ❤️ As a token of appreciation for sponsoring, you can unlock these awesome rewards and extra features 🦄✨\nEcosystem Hugo Academic CLI: Automatically import publications from BibTeX Inspiration Check out the latest demo of what you’ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\nFeatures Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 34+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files. Themes Wowchemy and its templates come with automatic day (light) and night (dark) mode built-in. Alternatively, visitors can choose their preferred mode - click the moon icon in the top right of the Demo to see it in action! Day/night mode can also be disabled by the site admin in params.toml.\nChoose a stunning theme and font for your site. Themes are fully customizable.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1607817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607817600,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://sandhyat.github.io/post/getting-started/","publishdate":"2020-12-13T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Welcome 👋 We know that first impressions are important, so we've populated your new site with some initial content to help you get familiar with everything in no time.","tags":["Academic","开源"],"title":"Welcome to Wowchemy, the website builder for Hugo","type":"post"},{"authors":["Sandhya Tripathi","N Hemachandra","Prashant Trivedi"],"categories":null,"content":"","date":1607558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607558400,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://sandhyat.github.io/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"While performing Feature Subset Selection (FSS) to identify important features, a weight is assigned to each feature that is not necessarily meaningful or interpretable w.r.t. final task and in turn leads to non-actionable information. To provide a solution to this problem of interpretable FSS, we introduce a novel notion of classification game with features as players and hinge loss based characteristic function. We use the Shapley value of this game to apportion the total training error to explicitly compute the contribution of each feature (Shapley Value based Error Apportioning, SVEA) to the total training error. We formalize the notion of interpret ability in FSS by identifying 3 final task related conditions. We empirically demonstrate that features with SVEA values less than zero are the dominant ones; this set is unique for a dataset as Shapley value is unique for a game instance. For the datasets that had negative apportioning, we observe a high value of the power of classification, P SV . It compares the performance of a set of linear and non-linear classifiers learned on Shapley value-based important features and the full feature set, in most of the cases. We customize a known Monte Carlo based approximation algorithm to avoid expensive Shapley value computations. We demonstrate the sample bias robustness of SVEA scheme by providing interval estimates. We illustrate all the above aspects on both synthetic and real datasets and showed that our scheme out-performs many existing approaches like recursive feature elimination and ReliefF in most of the cases.","tags":[],"title":"Interpretable feature subset selection: A Shapley value based approach","type":"publication"},{"authors":["N Hemachandra","Kishor Patil","Sandhya Tripathi"],"categories":null,"content":"","date":1607126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607126400,"objectID":"e742187ea36c14a88e5db8c319a32de4","permalink":"https://sandhyat.github.io/publication/journal-article1/","publishdate":"2020-12-05T00:00:00Z","relpermalink":"/publication/journal-article1/","section":"publication","summary":"Queues can be seen as a service facility where quality of service (QoS) is an important measure for the performance of the system. In many cases, the queue implements the optimal admission control (either discounted or average) policy in the presence of holding/congestion cost and revenue collected from admitted customers. In this paper, users offer an arrival rate at stationarity that depends on the QoS they experience. We study the interaction between arriving customers and such a queue under two different QoS measures—the asymptotic rate of the customers lost and the fraction of customers lost in the long run. In particular, we investigate the behaviour of equilibrium points and equilibrium sets associated with this interaction and their interpretations in terms of business cycles. We provide sufficient conditions for existence of equilibrium sets for M/M/1 queue. These conditions further help us to identify the relationship among system parameters for which equilibrium sets exist. Next, we consider GI/M/1 queues and provide a sufficient condition for existence of multiple optimal revenue policies. We then specialize these results to study the equilibrium sets of (i) a D/M/1 queue and (ii) a queue where the arrival rate is locally continuous. The equilibrium behaviour in the latter case is more interesting as there may be multiple equilibrium points or sets. Motivated by such queues, we introduce a weaker version of monotonicity and investigate the existence of generalized equilibrium sets.","tags":null,"title":"Equilibrium points and equilibrium sets of some  GI/M/1 queues","type":"publication"},{"authors":null,"categories":["R"],"content":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nYou can embed an R code chunk like this:\nsummary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.:19.0 3rd Qu.: 56.00 ## Max. :25.0 Max. :120.00 fit \u0026lt;- lm(dist ~ speed, data = cars) fit ## ## Call: ## lm(formula = dist ~ speed, data = cars) ## ## Coefficients: ## (Intercept) speed ## -17.579 3.932 Including Plots You can also embed plots. See Figure 1 for example:\npar(mar = c(0, 1, 0, 1)) pie( c(280, 60, 20), c(\u0026#39;Sky\u0026#39;, \u0026#39;Sunny side of pyramid\u0026#39;, \u0026#39;Shady side of pyramid\u0026#39;), col = c(\u0026#39;#0292D8\u0026#39;, \u0026#39;#F7EA39\u0026#39;, \u0026#39;#C4B632\u0026#39;), init.angle = -50, border = NA ) Figure 1: A fancy pie chart.\n","date":1606875194,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606875194,"objectID":"84a876ba789bb7232be8d9ed2487fd98","permalink":"https://sandhyat.github.io/post/2020-12-01-r-rmarkdown/","publishdate":"2020-12-01T21:13:14-05:00","relpermalink":"/post/2020-12-01-r-rmarkdown/","section":"post","summary":"R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.","tags":["R Markdown","plot","regression"],"title":"Hello R Markdown","type":"post"},{"authors":["Sandhya Tripathi","Bradley A Fritz","Mohamed Abdelhack","Michael S Avidan","Yixin Chen","Christopher R King"],"categories":null,"content":"","date":1604361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604361600,"objectID":"7e92ea7cc5057bac31a7b1f1dddbc3b7","permalink":"https://sandhyat.github.io/publication/preprint_fairness/","publishdate":"2020-11-03T00:00:00Z","relpermalink":"/publication/preprint_fairness/","section":"publication","summary":"With the current ongoing debate about fairness, explainability and transparency of machine learning models, their application in high-impact clinical decision-making systems must be scrutinized. We consider a real-life example of risk estimation before surgery and investigate the potential for bias or unfairness of a variety of algorithms. Our approach creates transparent documentation of potential bias so that the users can apply the model carefully. We augment a model-card like analysis using propensity scores with a decision-tree based guide for clinicians that would identify predictable shortcomings of the model. In addition to functioning as a guide for users, we propose that it can guide the algorithm development and informatics team to focus on data sources and structures that can address these shortcomings.","tags":null,"title":"(Un) fairness in Post-operative Complication Prediction Models","type":"publication"},{"authors":["Sandhya Tripathi","N Hemachandra"],"categories":null,"content":"","date":1603065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1603065600,"objectID":"cc45f1729da33d20f994a4e5c26da319","permalink":"https://sandhyat.github.io/publication/preprint_gans/","publishdate":"2020-10-19T00:00:00Z","relpermalink":"/publication/preprint_gans/","section":"publication","summary":"We use Generative Adversarial Networks (GANs) to design a class conditional label noise (CCN) robust scheme for binary classification. It first generates a set of correctly labelled data points from noisy labelled data and 0.1% or 1% clean labels such that the generated and true (clean) labelled data distributions are close; generated labelled data is used to learn a good classifier. The mode collapse problem while generating correct feature-label pairs and the problem of skewed feature-label dimension ratio ( 784:1) are avoided by using Wasserstein GAN and a simple data representation change. Another WGAN with information-theoretic flavour on top of the new representation is also proposed. The major advantage of both schemes is their significant improvement over the existing ones in presence of very high CCN rates, without either estimating or cross-validating over the noise rates. We proved that KL divergence between clean and noisy distribution increases w.r.t. noise rates in symmetric label noise model; can be extended to high CCN rates. This implies that our schemes perform well due to the adversarial nature of GANs. Further, use of generative approach (learning clean joint distribution) while handling noise enables our schemes to perform better than discriminative approaches like GLC, LDMI and GCE; even when the classes are highly imbalanced. Using Friedman F test and Nemenyi posthoc test, we showed that on high dimensional binary class synthetic, MNIST and Fashion MNIST datasets, our schemes outperform the existing methods and demonstrate consistent performance across noise rates.","tags":null,"title":"GANs for learning from very high class conditional noisy labels","type":"publication"},{"authors":["Sandhya Tripathi","N Hemachandra"],"categories":null,"content":"","date":1602115200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602115200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://sandhyat.github.io/talk/label-noise-problems-and-solutions/","publishdate":"2020-10-06T00:00:00Z","relpermalink":"/talk/label-noise-problems-and-solutions/","section":"event","summary":"This was a tutorial given at DSAA'2020 conference virtually.","tags":null,"title":"Label Noise: Problems and Solutions","type":"event"},{"authors":["Aditya Petety","Sandhya Tripathi","N Hemachandra"],"categories":null,"content":"","date":1585872000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585872000,"objectID":"69ea2d521456c13b2db5388ff88ee9cc","permalink":"https://sandhyat.github.io/publication/conference-paper4/","publishdate":"2020-04-03T00:00:00Z","relpermalink":"/publication/conference-paper4/","section":"publication","summary":"We consider the problem of learning linear classifiers when both features and labels are binary. In addition, the features are noisy, i.e., they could be flipped with an unknown probability. In Sy-De attribute noise model, where all features could be noisy together with same probability, we show that 0-1 loss ({{\u003c math \u003e}}$l_{0-1}${{\u003c /math \u003e}}) need not be robust but a popular surrogate, squared loss ({{\u003c math \u003e}}$l_{sq}${{\u003c /math \u003e}}) is. In Asy-In attribute noise model, we prove that {{\u003c math \u003e}}$l_{0-1}${{\u003c /math \u003e}} is robust for any distribution over 2 dimensional feature space. However, due to computational intractability of {{\u003c math \u003e}}$l_{0-1}${{\u003c /math \u003e}}, we resort to {{\u003c math \u003e}}$l_{sq}${{\u003c /math \u003e}} and observe that it need not be Asy-In noise robust. Our empirical results support Sy-De robustness of squared loss for low to moderate noise rates.","tags":[],"title":"Attribute Noise Robust Binary Classification (Student Abstract)","type":"publication"},{"authors":null,"categories":null,"content":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.\nOn this page, you’ll find some examples of the types of technical content that can be rendered with Wowchemy.\nExamples Code Wowchemy supports a Markdown extension for highlighting code syntax. You can customize the styles under the syntax_highlighter option in your config/_default/params.yaml file.\n```python import pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() ``` renders as\nimport pandas as pd data = pd.read_csv(\u0026#34;data.csv\u0026#34;) data.head() Mindmaps Wowchemy supports a Markdown extension for mindmaps.\nSimply insert a Markdown markmap code block and optionally set the height of the mindmap as shown in the example below.\nA simple mindmap defined as a Markdown list:\n```markmap {height=\u0026#34;200px\u0026#34;} - Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal ``` renders as\n- Hugo Modules - wowchemy - wowchemy-plugins-netlify - wowchemy-plugins-netlify-cms - wowchemy-plugins-reveal A more advanced mindmap with formatting, code blocks, and math:\n```markmap - Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ ``` renders as\n- Mindmaps - Links - [Wowchemy Docs](https://wowchemy.com/docs/) - [Discord Community](https://discord.gg/z8wNYzb) - [GitHub](https://github.com/wowchemy/wowchemy-hugo-themes) - Features - Markdown formatting - **inline** ~~text~~ *styles* - multiline text - `inline code` - ```js console.log(\u0026#39;hello\u0026#39;); console.log(\u0026#39;code block\u0026#39;); ``` - Math: $x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}$ Charts Wowchemy supports the popular Plotly format for interactive charts.\nSave your Plotly JSON in your page folder, for example line-chart.json, and then add the {{\u0026lt; chart data=\u0026#34;line-chart\u0026#34; \u0026gt;}} shortcode where you would like the chart to appear.\nDemo:\nYou might also find the Plotly JSON Editor useful.\nMath Wowchemy supports a Markdown extension for $\\LaTeX$ math. You can enable this feature by toggling the math option in your config/_default/params.yaml file.\nTo render inline or block math, wrap your LaTeX math with {{\u0026lt; math \u0026gt;}}$...${{\u0026lt; /math \u0026gt;}} or {{\u0026lt; math \u0026gt;}}$$...$${{\u0026lt; /math \u0026gt;}}, respectively. (We wrap the LaTeX math in the Wowchemy math shortcode to prevent Hugo rendering our math as Markdown. The math shortcode is new in v5.5-dev.)\nExample math block:\n{{\u0026lt; math \u0026gt;}} $$ \\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2} $$ {{\u0026lt; /math \u0026gt;}} renders as\n$$\\gamma_{n} = \\frac{ \\left | \\left (\\mathbf x_{n} - \\mathbf x_{n-1} \\right )^T \\left [\\nabla F (\\mathbf x_{n}) - \\nabla F (\\mathbf x_{n-1}) \\right ] \\right |}{\\left \\|\\nabla F(\\mathbf{x}_{n}) - \\nabla F(\\mathbf{x}_{n-1}) \\right \\|^2}$$ Example inline math {{\u0026lt; math \u0026gt;}}$\\nabla F(\\mathbf{x}_{n})${{\u0026lt; /math \u0026gt;}} renders as $\\nabla F(\\mathbf{x}_{n})$.\nExample multi-line math using the math linebreak (\\\\):\n{{\u0026lt; math \u0026gt;}} $$f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases}$$ {{\u0026lt; /math \u0026gt;}} renders as\n$$ f(k;p_{0}^{*}) = \\begin{cases}p_{0}^{*} \u0026amp; \\text{if }k=1, \\\\ 1-p_{0}^{*} \u0026amp; \\text{if }k=0.\\end{cases} $$ Diagrams Wowchemy supports a Markdown extension for diagrams. You can enable this feature by toggling the diagram option in your config/_default/params.toml file or by adding diagram: true to your page front matter.\nAn example flowchart:\n```mermaid graph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] ``` renders as\ngraph TD A[Hard] --\u0026gt;|Text| B(Round) B --\u0026gt; C{Decision} C --\u0026gt;|One| D[Result 1] C --\u0026gt;|Two| E[Result 2] An example sequence diagram:\n```mermaid sequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! ``` renders as\nsequenceDiagram Alice-\u0026gt;\u0026gt;John: Hello John, how are you? loop Healthcheck John-\u0026gt;\u0026gt;John: Fight against hypochondria end Note right of John: Rational thoughts! John--\u0026gt;\u0026gt;Alice: Great! John-\u0026gt;\u0026gt;Bob: How about you? Bob--\u0026gt;\u0026gt;John: Jolly good! An example Gantt diagram:\n```mermaid gantt section Section Completed :done, des1, 2014-01-06,2014-01-08 Active :active, des2, 2014-01-07, 3d Parallel 1 : des3, after des1, 1d Parallel 2 : des4, after des1, 1d Parallel 3 : des5, after des3, 1d Parallel 4 : des6, after des4, 1d ``` renders …","date":1562889600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562889600,"objectID":"07e02bccc368a192a0c76c44918396c3","permalink":"https://sandhyat.github.io/post/writing-technical-content/","publishdate":"2019-07-12T00:00:00Z","relpermalink":"/post/writing-technical-content/","section":"post","summary":"Wowchemy is designed to give technical content creators a seamless experience. You can focus on the content and Wowchemy handles the rest.\nHighlight your code snippets, take notes on math classes, and draw diagrams from textual representation.","tags":null,"title":"Writing technical content in Markdown","type":"post"},{"authors":["Sandhya Tripathi","N Hemachandra"],"categories":null,"content":"","date":1553212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553212800,"objectID":"1191acb343bec437a4d3f2972c18f2a5","permalink":"https://sandhyat.github.io/publication/conference-paper-copy/","publishdate":"2019-03-22T00:00:00Z","relpermalink":"/publication/conference-paper-copy/","section":"publication","summary":"In binary classification framework, we are interested in making cost sensitive label predictions in the presence of uniform/symmetric label noise. We first observe that 0–1 Bayes classifiers are not (uniform) noise robust in cost sensitive setting. To circumvent this impossibility result, we present two schemes; unlike the existing methods, our schemes do not require noise rate. The first one uses {{\u003c math \u003e}}$\\alpha${{\u003c /math \u003e}}-weighted {{\u003c math \u003e}}$\\gamma${{\u003c /math \u003e}}-uneven margin squared loss function, {{\u003c math \u003e}}$l_{\\alpha, usq}${{\u003c /math \u003e}} , which can handle cost sensitivity arising due to domain requirement (using user given {{\u003c math \u003e}}$\\alpha${{\u003c /math \u003e}}) or class imbalance (by tuning {{\u003c math \u003e}}$\\gamma${{\u003c /math \u003e}}) or both. However, we observe that {{\u003c math \u003e}}$l_{\\alpha, usq}${{\u003c /math \u003e}} Bayes classifiers are also not cost sensitive and noise robust. We show that regularized ERM of this loss function over the class of linear classifiers yields a cost sensitive uniform noise robust classifier as a solution of a system of linear equations. We also provide a performance bound for this classifier. The second scheme that we propose is a re-sampling based scheme that exploits the special structure of the uniform noise models and uses in-class probability estimates. Our computational experiments on some UCI datasets with class imbalance show that classifiers of our two schemes are on par with the existing methods and in fact better in some cases w.r.t. Accuracy and Arithmetic Mean, without using/tuning noise rate. We also consider other cost sensitive performance measures viz., F measure and Weighted Cost for evaluation.","tags":[],"title":"Cost Sensitive Learning in the Presence of Symmetric Label Noise","type":"publication"},{"authors":["Sandhya Tripathi"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Install Python and JupyterLab Install Anaconda which includes Python 3 and JupyterLab.\nAlternatively, install JupyterLab with pip3 install jupyterlab.\nCreate or upload a Jupyter notebook Run the following commands in your Terminal, substituting \u0026lt;MY-WEBSITE-FOLDER\u0026gt; and \u0026lt;SHORT-POST-TITLE\u0026gt; with the file path to your Academic website folder and a short title for your blog post (use hyphens instead of spaces), respectively:\nmkdir -p \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ cd \u0026lt;MY-WEBSITE-FOLDER\u0026gt;/content/post/\u0026lt;SHORT-POST-TITLE\u0026gt;/ jupyter lab index.ipynb The jupyter command above will launch the JupyterLab editor, allowing us to add Academic metadata and write the content.\nEdit your post metadata The first cell of your Jupter notebook will contain your post metadata (front matter).\nIn Jupter, choose Markdown as the type of the first cell and wrap your Academic metadata in three dashes, indicating that it is YAML front matter:\n--- title: My post\u0026#39;s title date: 2019-09-01 # Put any other Academic metadata here... --- Edit the metadata of your post, using the documentation as a guide to the available options.\nTo set a featured image, place an image named featured into your post’s folder.\nFor other tips, such as using math, see the guide on writing content with Academic.\nConvert notebook to Markdown jupyter nbconvert index.ipynb --to markdown --NbConvertApp.output_files_dir=. Example This post was created with Jupyter. The orginal files can be found at https://github.com/gcushen/hugo-academic/tree/master/exampleSite/content/post/jupyter\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://sandhyat.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in Academic using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with Academic","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://sandhyat.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Sandhya Tripathi","N Hemachandra"],"categories":null,"content":"","date":1515628800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1515628800,"objectID":"b057bc5f579dd349167e06419d10ca54","permalink":"https://sandhyat.github.io/publication/conference-paper3/","publishdate":"2018-01-11T00:00:00Z","relpermalink":"/publication/conference-paper3/","section":"publication","summary":"We first propose an empirical risk minimization based binary classi- fication algorithm, ExpERM, with exponential function as surrogate loss function. Our ExpERM algorithm is scalable in both the di- mension of feature space as well as in the number of data points as it is an unconstrained differentiable convex optimization prob- lem or a convex optimization problem with a single constraint in the regularized ExpERM framework. Under mild assumption on data, we show that ExpERM classifier is unique. We implement ExpERM on wide collection (large-features, large-examples) of real datasets. We use Wilcoxon signed-rank test to show that these easily computable classifiers have good generalization property as their 5 or 10-fold cross validation error is not significantly different from those classifiers obtained by standard hinge loss based SVMs or AdaBoost classifiers. Using some large feature sized datasets, we make a statistical comparison of SVM and ExpERM and observe that ExpERM takes remarkably less time to train the model. Towards better understanding this simple and effective learning algorithm, we obtain PAC like sample complexity bounds for reg- ularized ExpERM and high probability bounds for ExpERM based on Rademacher complexity and uniform stability notions. Our computational experience suggests that the regularization does not significantly improve the performance of the ExpERM based classifier. Due to repeated calls to be made to the binary classifier routines in implementation of combined multi-class algorithms, ExpERM scheme is expected to be significantly computationally cheaper and hence scalable. We statistically show that CV error of our binary ExpERM classifiers on many multi-class datasets is comparable to those obtained using computationally expensive binary class SVMs.","tags":[],"title":"Scalable linear classifiers based on exponential loss function","type":"publication"}]